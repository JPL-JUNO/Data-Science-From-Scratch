{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ch09_Getting_Data import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stdin and stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics of Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_for_reading = open('reading_file.txt', 'r')\n",
    "file_for_reading2 = open('reading_file.txt')\n",
    "\n",
    "file_for_writing = open('writing_file.txt', 'w')\n",
    "\n",
    "file_open_appending = open('appending_file.txt', 'a')\n",
    "# don't forget to close your files when you're done\n",
    "file_for_writing.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with open(filename) as f:\n",
    "    data = function_that_gets_data_from(f)\n",
    "    \n",
    "# at this point f has already been closed, so don't try to use it\n",
    "process(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_domain('cuixuanstephem@gmail.com') == 'gmail.com'\n",
    "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Web"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML and the Parsing Thereof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"https://raw.githubusercontent.com/joelgrus/data/master/getting-data.html\")\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "\n",
    "first_paragraph = soup.find('p')\n",
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "first_paragraph_id = soup.p['id']  # raises KeyError if no 'id'\n",
    "first_paragraph_id2 = soup.p.get('id')  # return None if no 'id'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently, you’ll want to find tags with a specific class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs = soup('p', {'class': 'important'})\n",
    "important_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"important\">This is the second paragraph.</p>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_paragraphs3 = [p for p in soup('p')\n",
    "                         if 'important' in p.get('class', [])]\n",
    "important_paragraphs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span id=\"name\">Joel</span>,\n",
       " <span id=\"twitter\">@joelgrus</span>,\n",
       " <span id=\"email\">joelgrus-at-gmail</span>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans_inside_divs = [span \n",
    "                     for div in soup('div')\n",
    "                     for span in div('span')]\n",
    "spans_inside_divs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Keeping Tabs on Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.house.gov/representatives\"\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(text, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls = [a['href']\n",
    "            for a in soup('a')\n",
    "            if a.has_attr('href')]\n",
    "len(all_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns way too many URLs. If you look at them, the ones we want start with either http:// or https://, have some kind of name, and end with either .house.gov or .house.gov/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
    "assert re.match(regex, \"http://joel.house.gov\")\n",
    "assert re.match(regex, \"https://joel.house.gov\")\n",
    "assert re.match(regex, \"http://joel.house.gov/\")\n",
    "assert re.match(regex, \"https://joel.house.gov/\")\n",
    "assert not re.match(regex, \"joel.house.gov\")\n",
    "assert not re.match(regex, \"http://joel.house.com\")\n",
    "assert not re.match(regex, \"https://joel.house.gov/biography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_urls = [url for url in all_urls if re.match(regex, url)]\n",
    "len(good_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the list, there are a lot of duplicates. Let’s use set to get rid of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_urls = list(set(good_urls))\n",
    "len(good_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://jayapal.house.gov/category/news/',\n",
       " 'https://jayapal.house.gov/category/press-releases/'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get('https://jayapal.house.gov').text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行时间较长，先注释掉\n",
    "# from typing import Dict, Set\n",
    "# press_releases: Dict[str, Set[str]] = {}\n",
    "\n",
    "# for house_url in good_urls:\n",
    "#     html = requests.get(house_url).text\n",
    "#     soup = BeautifulSoup(html, 'html5lib')\n",
    "#     pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "\n",
    "#     press_releases[house_url] = pr_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
    "assert paragraph_mentions(text, \"twitter\")\n",
    "assert not paragraph_mentions(text, \"facebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for house_url, pr_links in press_releases.items():\n",
    "#     for pr_link in pr_links:\n",
    "#         url = f'{house_url}/{pr_link}'\n",
    "#         text = requests.get(url).text\n",
    "#         if paragraph_mentions(text, 'data'):\n",
    "#             print(f'house_url')\n",
    "#             break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON and XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert deserialized['publicationYear'] == 2019\n",
    "assert \"data science\" in deserialized['topics']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Unauthenticated API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using the Twitter APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSFS-2nd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
